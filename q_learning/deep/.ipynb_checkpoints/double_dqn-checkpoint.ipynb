{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Deep Q-Networks](https://www.datahubbs.com/deep-q-learning-101/) are great, but they have a slight problem - they tend to overestimate their Q-values. To address this, we can extend the ideas developed in the [double Q-learning learning](https://www.datahubbs.com/double-q-learning/) case to DQN's. This gives us **Double Deep Q-Networks**, which use a second network to learn an unbiased estimation of the Q-values. The great thing about this, is that it can reduce the over-estimation which also [improves performance](https://arxiv.org/abs/1509.06461).\n",
    "\n",
    "## TL;DR\n",
    "\n",
    "We build a Double DQN system to reduce the bias in training to learn a simple CartPole task.\n",
    "\n",
    "## DQN Recap\n",
    "\n",
    "DQN's burst on the scene when the cracked the Atari code for DeepMind a few years back. The key was to take [Q-learning](https://www.datahubbs.com/intro-to-q-learning/), but estimate the Q-function with a deep neural network. Now, simply using the Q-learning update equation to change the weights and biases of a neural network wasn't quite enough, so a few tricks had to be introduced to assist the network and enable it to train consistently.\n",
    "\n",
    "The first trick was adding **experience replay**. This is a memory buffer that stores previously viewed state-action-reward-next state tuples that is sampled from during updating to break sequence dependence. \n",
    "\n",
    "The second trick was adding a **target network** to the system. This target network is just a copy of the neural network that we are training and gets updated periodically by the algorithm. The reason this is important is that it provides a relatively stable baseline for performance measurement. In reinforcement learning, we don't have labeled data to tell us when we're right or wrong, instead we have a reward signal. If we're maximizing a reward, then the bigger the better as far as our algorithm is concerned, but we don't know how big it gets.\n",
    "\n",
    "For example, your RL agent is chugging along and picking up a few rewards here and there while completing its task. Lots of 0's and 1's, then suddenly, it gets a big reward of 10. Is that good? Well, it's better than anything we've seen thus far for sure, but perhaps if we took a different action we would have gotten a reward of 50 or 100 instead. We just don't know. \n",
    "\n",
    "This is part of the challenge of RL, we have to explore and learn a good policy, but also learn a baseline to compare our performance against. This is where the target network comes in. This target serves as our baseline, but if our baseline changes too much, it becomes very hard to learn, so we only update it intermittently. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
