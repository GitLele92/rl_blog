{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Deep Q-Networks](https://www.datahubbs.com/deep-q-learning-101/) are great, but they have a slight problem - they tend to overestimate their Q-values. A very easy way to address this, is by extending the ideas developed in the [double Q-learning](https://www.datahubbs.com/double-q-learning/) case to DQN's. This gives us **Double Deep Q-Networks**, which use a second network to learn an unbiased estimation of the Q-values. The great thing about this, is that it can reduce the over-estimation which also [improves performance](https://arxiv.org/abs/1509.06461).\n",
    "\n",
    "## TL;DR\n",
    "\n",
    "We build a Double DQN system to reduce the bias in training to learn a simple CartPole task.\n",
    "\n",
    "## DQN Recap\n",
    "\n",
    "DQN's burst on the scene when the cracked the Atari code for DeepMind a few years back. The key was to take [Q-learning](https://www.datahubbs.com/intro-to-q-learning/), but estimate the Q-function with a deep neural network. Now, simply using the Q-learning update equation to change the weights and biases of a neural network wasn't quite enough, so a few tricks had to be introduced to assist the network and enable it to train consistently.\n",
    "\n",
    "The first trick was adding **experience replay**. This is a memory buffer that stores previously viewed state-action-reward-next state tuples that is sampled from during updating to break sequence dependence. \n",
    "\n",
    "The second trick was adding a **target network** to the system. This target network is just a copy of the neural network that we are training and gets updated periodically by the algorithm. The reason this is important is that it provides a relatively stable baseline for performance measurement. In reinforcement learning, we don't have labeled data to tell us when we're right or wrong, instead we have a reward signal. If we're maximizing a reward, then the bigger the better as far as our algorithm is concerned, but we don't know how big it gets.\n",
    "\n",
    "For example, your RL agent is chugging along and picking up a few rewards here and there while completing its task. Lots of 0's and 1's, then suddenly, it gets a big reward of 10. Is that good? Well, it's better than anything we've seen thus far for sure, but perhaps if we took a different action we would have gotten a reward of 50 or 100 instead. We just don't know. \n",
    "\n",
    "This is part of the challenge of RL, we have to explore and learn a good policy, but also learn a baseline to compare our performance against. This is where the target network comes in. This target serves as our baseline, but if our baseline changes too much, it becomes very hard to learn, so we only update it intermittently. \n",
    "\n",
    "## Leveraging the Target Network\n",
    "\n",
    "[Double Q-learning](https://www.datahubbs.com/double-q-learning/) is able to learn a better estimate because of its second Q-function approximator. We can do the same, but instead of a second, independent DQN, we can just tap our target network for double duty by using it to unbiase our estimates. This is quite easy to do by using the target network to estimate the value of our action. \n",
    "\n",
    "For a typical DQN, we calculate the loss using:\n",
    "\n",
    "$$y_t^{DQN} = R_t + \\gamma max_a \\big(Q(s_{t+1}; \\theta_T) \\big)$$\n",
    "  \n",
    "Where $\\theta_T$ represents our target network (and $\\theta$ our DQN). So here, we're getting an estimate of the value from our target network. To transform this into our value estimate for the double DQN ($y_t^{DDQN}$), we simply update the function as follows:\n",
    "\n",
    "$$y_t^{DDQN} = R_t + \\gamma Q\\big(s_{t+1}, argmax_a Q(s_{t+1}, a; \\theta), \\theta_T \\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference here, is we're using our DQN to get our maximum action, then passing that action into the target network to yield our value estimate. In this way, we've now changed our target value to take the best from both networks just like we did in the [tabular version](https://www.datahubbs.com/double-q-learning/).\n",
    "\n",
    "The full, DDQN algorithm is exactly the same as the DQN, just with an updated loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Initialize replay memory $D$ with $M$ samples and select minibatch sample size $B$\n",
    "\n",
    "> Initialize networks with weights $\\theta$ and $\\theta_t = \\theta$\n",
    "\n",
    "> Select parameters $\\alpha, \\gamma \\in (0, 1]$\n",
    "\n",
    "> **FOR** each episode:\n",
    "\n",
    ">> Initialize $s_0$\n",
    "\n",
    ">>> **FOR** each step $t$ in the episode:\n",
    "\n",
    ">>>> **IF** $p < \\epsilon$ select a random action $a_t$\n",
    "\n",
    ">>>> **ELSE** select $argmax_a \\big(Q(s_t; \\theta) \\big)$\n",
    "\n",
    ">>>> Take action $a_t$ and observe reward $R_t$ and new state $s_{t+1}$\n",
    "\n",
    ">>>> Store transition ($s_t$, $a_t$, $R_t$, $s_{t+1}$) in replay buffer $D$\n",
    "\n",
    ">>>> Sample random minibatch of $B$ transitions from $D$\n",
    "\n",
    ">>>> Calculate the loss for all samples: \n",
    "$$y_t^{DDQN} = R_t + \\gamma Q\\big(s_{t+1}, argmax_a Q(s_{t+1}, a; \\theta), \\theta_T \\big)$$\n",
    "$$\\mathcal{L}(\\theta) = \n",
    "\\begin{cases}\n",
    "  \\bigg(R_t + y_t^{DDQN} - Q(s_t, a_t; \\theta) \\bigg)^2 \\\\    \n",
    "  \\bigg(R_t - Q(s_t, a_t; \\theta) \\bigg)^2 \\quad \\text{if } s_{t+1} \\text{ is a terminal state}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    ">>>> Update parameters $\\theta$ with gradient descent\n",
    "\n",
    ">>>> Every $N$ steps, set $\\theta_T = \\theta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDQN Implementation\n",
    "\n",
    "Our new and improved algorithm uses all of the same parts as the original DQN, so for brevity, I'm not going to reproduce the code for the `QNetwork` or `experienceReplayBuffer` here. You can get those details on the [DQN post](https://www.datahubbs.com/deep-q-learning-101/) or on [GitHub](https://github.com/hubbs5/rl_blog/blob/master/q_learning/deep/ddqn.py). Here, we'll just run the new algorithm for our double DQN agent.\n",
    "\n",
    "So the only change here, is to the `calculate_loss` method where we have our new, DDQN loss calculation as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    \n",
    "    def __init__(self, env, network, buffer, epsilon=0.05, batch_size=32):\n",
    "        \n",
    "        self.env = env\n",
    "        self.network = network\n",
    "        self.target_network = deepcopy(network)\n",
    "        self.buffer = buffer\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.window = 100\n",
    "        self.reward_threshold = 195 # Avg reward before CartPole is \"solved\"\n",
    "        self.initialize()\n",
    "    \n",
    "    def take_step(self, mode='train'):\n",
    "        if mode == 'explore':\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = self.network.get_action(self.s_0, epsilon=self.epsilon)\n",
    "            self.step_count += 1\n",
    "        s_1, r, done, _ = self.env.step(action)\n",
    "        self.rewards += r\n",
    "        self.buffer.append(self.s_0, action, r, done, s_1)\n",
    "        self.s_0 = s_1.copy()\n",
    "        if done:\n",
    "            self.s_0 = env.reset()\n",
    "        return done\n",
    "        \n",
    "    # Implement DQN training algorithm\n",
    "    def train(self, gamma=0.99, max_episodes=10000, \n",
    "              batch_size=32,\n",
    "              network_update_frequency=4,\n",
    "              network_sync_frequency=2000):\n",
    "        self.gamma = gamma\n",
    "        # Populate replay buffer\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            self.take_step(mode='explore')\n",
    "            \n",
    "        ep = 0\n",
    "        training = True\n",
    "        while training:\n",
    "            self.s_0 = self.env.reset()\n",
    "            self.rewards = 0\n",
    "            done = False\n",
    "            while done == False:\n",
    "                done = self.take_step(mode='train')\n",
    "                # Update network\n",
    "                if self.step_count % network_update_frequency == 0:\n",
    "                    self.update()\n",
    "                # Sync networks\n",
    "                if self.step_count % network_sync_frequency == 0:\n",
    "                    self.target_network.load_state_dict(\n",
    "                        self.network.state_dict())\n",
    "                    self.sync_eps.append(ep)\n",
    "                    \n",
    "                if done:\n",
    "                    ep += 1\n",
    "                    self.training_rewards.append(self.rewards)\n",
    "                    self.training_loss.append(np.mean(self.update_loss))\n",
    "                    self.update_loss = []\n",
    "                    mean_rewards = np.mean(\n",
    "                        self.training_rewards[-self.window:])\n",
    "                    self.mean_training_rewards.append(mean_rewards)\n",
    "                    print(\"\\rEpisode {:d} Mean Rewards {:.2f}\\t\\t\".format(\n",
    "                        ep, mean_rewards), end=\"\")\n",
    "                    \n",
    "                    if ep >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "                    if mean_rewards >= self.reward_threshold:\n",
    "                        training = False\n",
    "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                            ep))\n",
    "                        break\n",
    "                        \n",
    "    def calculate_loss(self, batch):\n",
    "        states, actions, rewards, dones, next_states = [i for i in batch]\n",
    "        rewards_t = torch.FloatTensor(rewards).to(device=self.network.device).reshape(-1,1)\n",
    "        actions_t = torch.LongTensor(np.array(actions)).reshape(-1,1).to(\n",
    "            device=self.network.device)\n",
    "        dones_t = torch.ByteTensor(dones).to(device=self.network.device)\n",
    "        \n",
    "        qvals = torch.gather(self.network.get_qvals(states), 1, actions_t)\n",
    "        \n",
    "        #################################################################\n",
    "        # DDQN Update\n",
    "        next_actions = torch.max(self.network.get_qvals(next_states), dim=-1)[1]\n",
    "        next_actions_t = torch.LongTensor(next_actions).reshape(-1,1).to(\n",
    "            device=self.network.device)\n",
    "        target_qvals = self.target_network.get_qvals(next_states)\n",
    "        qvals_next = torch.gather(target_qvals, 1, next_actions_t).detach()\n",
    "        #################################################################\n",
    "        qvals_next[dones_t] = 0 # Zero-out terminal states\n",
    "        expected_qvals = self.gamma * qvals_next + rewards_t\n",
    "        loss = nn.MSELoss()(qvals, expected_qvals)\n",
    "        return loss\n",
    "    \n",
    "    def update(self):\n",
    "        self.network.optimizer.zero_grad()\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size)\n",
    "        loss = self.calculate_loss(batch)\n",
    "        loss.backward()\n",
    "        self.network.optimizer.step()\n",
    "        if self.network.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())\n",
    "        \n",
    "    def initialize(self):\n",
    "        self.training_rewards = []\n",
    "        self.training_loss = []\n",
    "        self.update_loss = []\n",
    "        self.mean_training_rewards = []\n",
    "        self.sync_eps = []\n",
    "        self.rewards = 0\n",
    "        self.step_count = 0\n",
    "        self.s_0 = self.env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our new `DDQNAgent`, we can go ahead and test it to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "buffer = experienceReplayBuffer(memory_size=10000, burn_in=1000)\n",
    "ddqn = QNetwork(env, learning_rate=1e-3)\n",
    "agent = DDQNAgent(env, ddqn, buffer)\n",
    "agent.train(max_episodes=5000, network_update_frequency=4, \n",
    "            network_sync_frequency=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It learns!\n",
    "\n",
    "<img src=\"https://www.datahubbs.com/wp-content/uploads/2019/09/ddqn_rewards.png\">\n",
    "\n",
    "You can also compare it to the performance of a DQN, as was done in the original paper. \n",
    "\n",
    "<img src=\"https://www.datahubbs.com/wp-content/uploads/2019/09/ddqn_paper_results.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can see that the value estimates (top row) are lower for the DDQN than the DQN. Additionally, they got better results with the DDQN thanks to these more accurate value estimates. \n",
    "\n",
    "DDQN is an easy way to boost the performance of your DQN, so go ahead, give it a shot!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
