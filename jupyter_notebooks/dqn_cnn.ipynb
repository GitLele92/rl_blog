{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Values from Colors\n",
    "\n",
    "Typically, I don't do much RL from pixels or images on this blog because, 1) my research doesn't deal with image processing, and more importantly 2) it takes a lot of compute! The extra compute is carried by the **Convolutional Neural Network** (CNN) that the image must be passed through first before it is fed to the fully-connected layer. Think about the task we're setting before our neural network for a moment. It not only has to learn how to play a game with no prior models to assist it, but it also has to learn relationships between all of those pixels and how they change through time, again, with no prior model. No wonder it takes so many computational resources to train!\n",
    "\n",
    "Starting by learning to [play CartPole with a DQN](https://www.datahubbs.com/deep-q-learning-101/) is a good way to test your algorithm for bugs, here we'll push it to do more by following the [DeepMind paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) to play Atari from the pixels on the screen. Just by \"watching\" the screen and making movements, these algorithms were able to acheive the impressive accomplishment of surpassing human performance for many games.\n",
    "\n",
    "## TL;DR\n",
    "\n",
    "We build a DQN with a convolutional neural network (CNN) in order to learn to play from the pixels on the screen. This is the way a lot of games are played with deep reinforcement learning and makes these techniques applicable to images.\n",
    "\n",
    "<img src=\"https://i0.wp.com/www.datahubbs.com/wp-content/uploads/2019/02/atari_games.png?w=790&ssl=1\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of CNN's\n",
    "\n",
    "To go into depth on CNN's would take us far afield here (see this post [here](https://skymind.ai/wiki/convolutional-network) and video [here](https://www.youtube.com/watch?v=FmpDIaiMIeA) for more), but we can cover the essential basics that you need to grasp the workings of the DQN algorithm. \n",
    "\n",
    "Convolutional networks are used in image recognition tasks and typically take three-dimensional tensors as inputs. Images have a height and width (two of the dimensions) while the third is devoted to the **RGB color**. This means for each color we see, we can represent it digitally as a combination of three colors: red, green, and blue. For CNN's, these input dimensions are commonly referred to as **channels**.\n",
    "\n",
    "<img src=\"https://www.datahubbs.com/wp-content/uploads/2019/04/3dmatrix.png\">\n",
    "\n",
    "The image is transformed via a **kernel** (also called a **filter**). If you have an RGB image, then you have one filter being applied for each color channel of your image. \n",
    "\n",
    "In the picture below (borrowed from [cs231n notes](http://cs231n.github.io/convolutional-networks/) which has a handy animation), the kernel moves across the channel and calculates the dot product of the corresponding values. This is done for all the pixels where we move a given number of pixels at a time, called the **stride**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.datahubbs.com/wp-content/uploads/2019/04/karpathy-convnet-labels.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is challenging to clearly explain what is going on in the image above, so stick with me and we'll walk through a computational example for the highlighted portion. The blue and gray matrices on the left are our inputs (`x`) with each channel (0, 1, 2) corresponding to some RGB value. The gray boxes represent the **padding** that's applied to each image, which is simply 0 values or whitespace around the images we're processing. We apply our filters/kernels (red matrices) to each of those in turn, then sum those values and add the bias to get our output (green matrices). We can calculate the example above with `numpy` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0:\n",
      "[[2 1 0]\n",
      " [1 2 0]\n",
      " [2 1 0]]\n",
      "f0:\n",
      "[[1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]]\n",
      "x1:\n",
      "[[2 0 0]\n",
      " [2 2 0]\n",
      " [1 0 0]]\n",
      "f1:\n",
      "[[ 0 -1  0]\n",
      " [ 0  1  1]\n",
      " [-1  1 -1]]\n",
      "x2:\n",
      "[[0 2 0]\n",
      " [2 1 0]\n",
      " [1 0 0]]\n",
      "f2:\n",
      "[[ 1  0  1]\n",
      " [-1  1  0]\n",
      " [-1  1  1]]\n",
      "Output value: 9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x0 = np.array([[2, 1, 0], [1, 2, 0], [2, 1, 0]])\n",
    "f0 = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n",
    "x1 = np.array([[2, 0, 0], [2, 2, 0], [1, 0, 0]])\n",
    "f1 = np.array([[0, -1, 0], [0, 1, 1], [-1, 1, -1]]) \n",
    "x2 = np.array([[0, 2, 0], [2, 1, 0], [1, 0, 0]])\n",
    "f2 = np.array([[1, 0, 1], [-1, 1, 0], [-1, 1, 1]])\n",
    "b = 1\n",
    "print(\"x0:\\n{}\".format(x0))\n",
    "print(\"f0:\\n{}\".format(f0))\n",
    "print(\"x1:\\n{}\".format(x1))\n",
    "print(\"f1:\\n{}\".format(f1))\n",
    "print(\"x2:\\n{}\".format(x2))\n",
    "print(\"f2:\\n{}\".format(f2))\n",
    "\n",
    "dot0 = np.dot(x0.flatten(), f0.flatten())\n",
    "dot1 = np.dot(x1.flatten(), f1.flatten())\n",
    "dot2 = np.dot(x2.flatten(), f2.flatten())\n",
    "out = dot0 + dot1 + dot2 + b\n",
    "print(\"Output value: {}\".format(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local and kernel values are stretched into columns (hence the `flatten()` method) before the dot product is calculated. By calculating each this way, our convolution is equivalent to a single, large matrix multiplication for the dot product between every kernel and pixel location. These values are given as our output matrix. \n",
    "\n",
    "We can see how these inputs get transformed and operated on through the convolutional layer, but what do their output dimensions look like? Thankfully, we have some handy formulas for this.\n",
    "\n",
    "If we have a 2D image (plus a color channel) we have a $W_{in} \\times H_{in} \\times D_{in}$ input. We convolve our images with the kernel which has its size $K$ ($K=3$ in our example above), number of kernels, $F$, stride $S$, and padding $P$. The output dimensions are then given as $W_{out} \\times H_{out} \\times D_{out}$ and are calculated as:\n",
    "\n",
    "$$W_{out} = \\frac{W_{in} - K + 2P}{S}+1$$\n",
    "\n",
    "$$H_{out} = \\frac{H_{in} - K + 2P}{S}+1$$\n",
    "\n",
    "$$D_{out} = F$$\n",
    "\n",
    "## Pooling Layer\n",
    "\n",
    "After convolving the input, there typically follows a **max pooling** layer. This operator is relatively simple compared to what we just examined. It has some kernel size and stride like we saw before, but rather than taking the dot product of the kernel, it returns the maximum value in the kernel window. Thus it creates a new matrix of maximum values of the various subsets it scans as shown in the color-coded image below.\n",
    "\n",
    "<img src=\"https://www.datahubbs.com/wp-content/uploads/2019/04/MaxpoolSample2.png\">\n",
    "\n",
    "Although this function wasn't applied in the DeepMind paper we'll be modeling our network off of, it is very common when building CNN's. \n",
    "\n",
    "The important takeaway is to understand a bit of what is going on in the convolutional portion of the neural network, particularly because the dimensions are so important to get right when constructing these things. Take it from my experience, it can be very frustrating to build a functioning CNN without a good understanding of why it fits together the way it does.\n",
    "\n",
    "To get the hang of putting some of these layers together in PyTorch, let's just build a simple example CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a CNN\n",
    "\n",
    "We can play with any of the [Atari environments](https://gym.openai.com/envs/#atari) in OpenAI Gym to get a feel for how this works. \n",
    "\n",
    "If you need to install the Atari environments, it's just:\n",
    "\n",
    "`pip install gym[atari]`\n",
    "\n",
    "Last I checked, the OpenAI Gym Atari environments won't run on a Windows machine. It appears there are [workarounds](https://stackoverflow.com/questions/42605769/openai-gym-atari-on-windows), but I haven't tried them myself. If you want to go ahead with this on Windows, then give that a shot or, finally do what the cool people do and grab a [Linux distro](https://www.techradar.com/news/best-linux-distro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "state = env.reset()\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(state)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.datahubbs.com/wp-content/uploads/2019/04/breakout_s_0.png\">\n",
    "\n",
    "I imported *Breakout* and showed a starting state with `matplotlib`. From here, we can inspect the `state`, which is just a 3D numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put together our single-layer convolutional network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=32, kernel_size=8, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be able to predict our output dimension from our above settings:\n",
    "\n",
    "$$H_{out} = \\frac{210 - 8 + 2\\times1}{2}+1 = 103$$\n",
    "\n",
    "$$W_{out} = \\frac{160 - 8 + 2\\times1}{2}+1 = 78$$\n",
    "\n",
    "PyTorch expects your color channel to be first for your input, so we need to re-organize a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 210, 160)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_state = np.moveaxis(state, 2, 0)\n",
    "_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.moveaxis` does what the name implies, allows us to swap axes around to our heart's content. \n",
    "\n",
    "The next adjustment we need to do is add a fourth dimension to the front of our array. This doesn't affect the image, but simply allows us to pass multiple images to the network at once in a single **batch**. Typically we'll do this with something like `np.stack([])` to combine states into batches, but in this case we'll just use `np.newaxis` because we've only got one observation to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "_state = _state[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, pass this to our `CNN` object and we can check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 103, 78])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN(torch.FloatTensor(_state)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions match up according to our prediction and we got no errors, so we're good! \n",
    "\n",
    "Before we get to building our DQN, we need to understand a bit about the steps DeepMind took to enable their DQN algorithm to learn from the Atari inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Pixels\n",
    "\n",
    "Learning to play from pixels isn't as straightforward as plugging the raw values into a network and letting it run. You can try that, but it's going to be costly in terms of compute (and by extension, in terms of your [AWS bill](https://amzn.to/2GevJdg)). We're going to help our algorithm along by applying a bit of pre-processing like found in the [DeepMind paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf).\n",
    "\n",
    "Let's refer back to our starting state image.\n",
    "\n",
    "<img src=\"https://www.datahubbs.com/wp-content/uploads/2019/04/breakout_s_0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we've got lots of colors and a lot of extra space and resolution. We can reduce our computational load by reducing the resolution and adjust the colors. DeepMind took the maximum pixel value over subsequent frames to reduce flickering caused by the limitations of the Atari platform and then scale it from its current $210 \\times 160 \\times 3$ resolution to $84 \\times 84$. We'll do something similar, except convert the colors to grayscale rather than adjust based on the maximimum pixel value.\n",
    "\n",
    "To convert this, we will take the luminance channel (denoted as $Y$) from the image, which is the our RGB channel, and apply linear weights to the channel to transform it according to the [relative luminance](https://en.wikipedia.org/wiki/Grayscale#Luma_coding_in_video_systems). \n",
    " \n",
    "$$Y = 0.299R + 0.587G + 0.114B$$\n",
    "\n",
    "We can do this easily with a small function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_lumininance(img):\n",
    "    return np.dot(img[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "s_g = scale_lumininance(s)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(s_g, cmap=plt.get_cmap('gray'))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.datahubbs.com/wp-content/uploads/2019/04/breakout_s_0_bw.png\">\n",
    "\n",
    "Now we've got our grayscale images and we've reduced our dimensionality by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image dimensions (210, 160, 3)\n",
      "Gray image dimensions (210, 160)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original image dimensions {}\".format(s.shape))\n",
    "print(\"Gray image dimensions {}\".format(s_g.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to go from $210 \\times 160$ down to $84 \\times 84$. I've found the easiest way to do this is with [`scikit-image`](http://scikit-image.org/docs/stable/auto_examples/transform/plot_rescale.html). Import this, and use the `transform.resize()` function and you'll quickly downsample your image to whatever value you specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84)\n"
     ]
    }
   ],
   "source": [
    "from skimage import transform\n",
    "\n",
    "s_g84 = transform.resize(s_g, (84, 84))\n",
    "\n",
    "print(s_g84.shape)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(s_g84, cmap=plt.get_cmap('gray'))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.datahubbs.com/wp-content/uploads/2019/04/breakout_s_0_resize.png\">\n",
    "\n",
    "Let's go ahead and tie these pre-processing steps together for our convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observations(obs):\n",
    "    obs_gray = scale_lumininance(obs)\n",
    "    obs_trans = transform.resize(obs_gray, (84, 84))\n",
    "    return np.moveaxis(obs_trans, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, before we can move on, there's one more element to discuss.\n",
    "\n",
    "Take a look at the photo below:\n",
    "\n",
    "<img src=\"https://www.datahubbs.com/wp-content/uploads/2019/03/ball-catch.jpg\">\n",
    "\n",
    "I can't tell if the ball is moving up or down; whether the girl just tossed the ball or is getting ready to catch it. But if you were to give me at least two sequential images, then I would be able to tell which direction the ball is moving. The same principle applies for a game like *Breakout*. Without this sequence of information, the agent won't be able to tell which direction the ball is moving.[<sup>1</sup>](#fn1)\n",
    "\n",
    "To help out, I'm going to introduce a parameter `tau` ($\\tau$) which is set to 4 by default. This means that each state we pass to the agent will actually have 4 frames stacked together so that it can pick up on the direction of motion. \n",
    "\n",
    "We'll link these frames together using `deque` from the `collections` package and set up two, one for the current state (`state_buffer`) and one for the next state (`next_state_buffer`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "tau = 4\n",
    "state_buffer = deque(maxlen=tau)\n",
    "next_state_buffer = deque(maxlen=tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the DQN\n",
    "\n",
    "With our pre-processing steps out of the way, we can turn to constructing our Deep Q-Network. We're going to skip ahead to the network and the implementation ([see this post if you want to see the algorithm](https://www.datahubbs.com/deep-q-learning-101/)). \n",
    "\n",
    "DeepMind placed their model architecture in the *Methods* section at the end of the paper along with most of the implementation details. They used a three-layer CNN followed by a fully-connected network consisting of one hidden layer and one output layer. \n",
    "\n",
    "|Layer|Kernel Size|Stride|Hidden Nodes|Activation Function|\n",
    "|-----|-----------|------|------------|-------------------|\n",
    "|CNN1|8|4|NA|ReLU|\n",
    "|CNN2|4|2|NA|ReLU|\n",
    "|CNN3|3|1|NA|ReLU|\n",
    "|FC1 |NA|NA|512|ReLU|\n",
    "\n",
    "To implement this in PyTorch, we stack our layers with the activation function in between as we would for any other network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN layers\n",
    "cnn = nn.Sequential(\n",
    "    nn.Conv2d(tau, 32, kernel_size=8, stride=4),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "    nn.ReLU()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first input is given by our $\\tau$ value to ensure we have the proper number of input channels. \n",
    "\n",
    "Unfortunately, we can't just slap our fully connected layer at the end because the dimensions won't match. Because the output of our CNN is a multi-dimensional tensor and a fully connected linear layer can only handle a 2D input, we need to transform it before passing it to the final layers. We have two options: use the CNN dimensionality equations we discussed above to calculate the dimensions, or let PyTorch do the work for us. Personally, I prefer the latter. \n",
    "\n",
    "We can get the answer from our network by passing in a sample state and simply looking at the output dimensions. In our case, we're resizing everything to an $84\\times84$ input, so we'll put $\\tau$ arrays together in our `state_buffer`, stack it, and send it through the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "input_dim = (84, 84)\n",
    "[state_buffer.append(np.zeros(input_dim)) for i in range(tau)]\n",
    "state_t = torch.FloatTensor(np.stack([state_buffer]))\n",
    "output = cnn(state_t)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we flatten our output tensor, we get our answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3136\n"
     ]
    }
   ],
   "source": [
    "fc_input_dim = output.flatten().shape[0]\n",
    "print(fc_input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need to make this work is a function that will take our CNN output, transform it according to the proper dimensions, and then pass that to our fully connected layers and we've got our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0086,  0.0376, -0.0117,  0.0186]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fully_connected = nn.Sequential(\n",
    "    nn.Linear(fc_input_dim, 512, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, env.action_space.n))\n",
    "\n",
    "def get_qvals(state):\n",
    "    state_t = torch.FloatTensor(state)\n",
    "    cnn_out = cnn(state_t).reshape(-1, fc_input_dim)\n",
    "    return fully_connected(cnn_out)\n",
    "\n",
    "get_qvals(np.stack([state_buffer]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this together into a `QNetwork` class is rather straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, env, learning_rate=1e-3, \n",
    "        tau=4, device='cpu', input_dim=(84,84), *args, **kwargs):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.device = device\n",
    "        self.actions = np.arange(env.action_space.n)\n",
    "        self.tau = tau\n",
    "        self.n_outputs = env.action_space.n\n",
    "\n",
    "        # CNN modeled off of Mnih et al.\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(tau, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc_layer_inputs = self.cnn_out_dim(input_dim)\n",
    "        \n",
    "        self.fully_connected = nn.Sequential(\n",
    "            nn.Linear(self.fc_layer_inputs, 512, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.n_outputs))\n",
    "        \n",
    "        # Set device for GPU's\n",
    "        if self.device == 'cuda':\n",
    "            self.cnn.cuda()\n",
    "            self.fully_connected.cuda()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                          lr=learning_rate)\n",
    "                \n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            action = self.greedy_action(state)\n",
    "        return action\n",
    "    \n",
    "    def greedy_action(self, state):\n",
    "        qvals = self.get_qvals(state)\n",
    "        return torch.max(qvals, dim=-1)[1].item()\n",
    "    \n",
    "    def get_qvals(self, state):\n",
    "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
    "        cnn_out = self.cnn(state_t).reshape(-1, self.fc_layer_inputs)\n",
    "        return self.fully_connected(cnn_out)\n",
    "\n",
    "    def cnn_out_dim(self, input_dim):\n",
    "        return self.cnn(torch.zeros(1, self.tau, *input_dim)\n",
    "            ).flatten().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just to show you it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0019,  0.0051,  0.0113, -0.0224]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = QNetwork(env)\n",
    "dqn.get_qvals(np.stack([state_buffer]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, we can plug this new network into the [previous DQN algorithm](https://www.datahubbs.com/deep-q-learning-101/) with only one small alteration, passing our states through the `preprocess_observation` function before loading them in the `state_buffer`. Rather than re-create the full algorithm here, you can build it yourself or just pull the CNN version from [GitHub](https://github.com/hubbs5/rl_blog/blob/master/q_learning/deep/dqn_cnn.py). If you want to run the algorithm with the same hyperparameters as DeepMind did, I've inserted the table from their paper below. \n",
    "\n",
    "Be warned, because this can take some time to train! The code works, but it could take 2-3 days to reach a reasonable level of performance for each game depending on your hardware configuration. GPU's definitely help speed the training, but neither the hyperparameters nor the code is optimized for fast performance, instead, I wrote it for clarity. Keep an eye out for a future post where we'll dive into parallelization techniques, profiling, and hyperparameter optimization to get the most out of your hardware.\n",
    "\n",
    "Between this post and the last (and lots of time and computational resources), you should be able to build a DQN to reproduce DeepMind's groundbreaking results in *Nature*. Good luck!\n",
    "\n",
    "<img src=\"https://www.datahubbs.com/wp-content/uploads/2019/04/hyperparameters.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"fn1\">1. [Mnih et al.](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) refer to this as the *agent history length*. It's also worth noting that using this type of concatenation strategy to capture motion is critical for learning from pixels such as is done in the original paper. There are also RAM environments that we can train on for most Atari games. If we were to use these, then it may not be necessary because to stack these frames together because it is likely that there is a value in the state that defines the direction of travel for the ball. Setting $\\tau=1$ will allow you to test that.</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
