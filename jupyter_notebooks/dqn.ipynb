{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Easy Intro to Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four years ago the nascent AI revolution moved out of image and natural language processing by introducing deep reinforcement learning to the scene. It came through a [paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) published in *Nature* by [DeepMind](https://deepmind.com/research/dqn/) showing how a deep neural network was able to surpass human-level performance on many of the Atari games available. It's one network learning to play dozens of games from the raw pixels on the screen. It was a major achievement and was based on **Deep Q-Learning**: [Q-learning](https://www.datahubbs.com/intro-to-q-learning/) with a deep neural network.\n",
    "\n",
    "## TL;DR\n",
    "We build a deep Q-learning model with a feed forward network to play OpenAI Gym environments based on the DeepMind algorithm and apply it to `CartPole` for computational ease (the next post will use CNN's to learn directly from pixels).\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://www.datahubbs.com/wp-content/uploads/2019/02/atari_games.png\">\n",
    "<figcaption>From Mnih et al. *Nature*</figcaption>\n",
    "</center></figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Networks\n",
    "\n",
    "Q-learning is predicated upon learning Q-values - i.e. the value of taking a given action when in a given state. Deep Q-Networks (DQN's) are no different in principal from [tabular Q-learning](https://www.datahubbs.com/intro-to-q-learning/), but instead of storing all of our Q-values in a look-up table, we represent them as a neural network. This allows for greater powers of generalization and a richer representation. Take an example from the [Atari game *Breakout*](https://gym.openai.com/envs/#atari), how would you represent the following states in a table?\n",
    "\n",
    "<img src=\"https://www.datahubbs.com/wp-content/uploads/2019/02/breakout1.png\">\n",
    "<img src=\"https://www.datahubbs.com/wp-content/uploads/2019/02/breakout2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with high-dimensional arrays, it is not a straightforward process to take these images and transpose them neatly into a state-action array to store Q-values. Moreover, if you were to go about encoding that, how is this going to translate to other games with different state representations such as shown in the DeepMind paper? To get around this, we'll abandon the tabular Q-learning approach and turn to a representation using [deep neural networks](https://www.datahubbs.com/deep-learning-101-first-neural-network-with-pytorch/) and an algorithm known as Deep Q-Learning.\n",
    "\n",
    "The basics of this approach are almost identical to [Q-learning](https://www.datahubbs.com/intro-to-q-learning/), but the neural network we use in Deep Q-Learning serves as a Q-value function approximation. All this means is we put in the state, and we get our best estimate of the value of taking each action as our output. Think of the neural network as a function that takes states and outputs a number corresponding to how good each action is. That's all it's really doing! Despite this simple function, it's a very powerful tool.\n",
    "\n",
    "## Target Networks and Experience Replay\n",
    "\n",
    "Moving to a DQN representation for Q-learning does provide a few difficulties that the tabular versions don't have to deal with. This is caused by the nonlinear deep neural network that we use as a function approximator and can be caused by sequence dependent correlations in the data and frequent updates to your Q approximation. The correlations occur because you move through an episode taking an action $a$ at each time step $t$. The reward you get at time $t$ ($R_t$) is highly correlated to the state and action at $t-1$, which in turn is related to the state and action at $t-2$ and so on. This path dependence makes it difficult to train a DQN. \n",
    "\n",
    "This is dealt with using **experience replay**: a memory bank of different states, actions, and rewards that we sample from randomly. This random sampling breaks any sequence dependence in the data when we update the network smoothing out the learning. There's a neuroscience connection here too because something like experience replay is thought to occur in the brain to [encode long-term memories](https://www.sciencedirect.com/science/article/pii/S0166223610000172). \n",
    "\n",
    "The second source of instability arises from your estimate of the next state. In tabular Q-learning, you update your values according to the following equation:\n",
    "\n",
    "$$Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\big(R_t + \\gamma max_a Q(s_{t+1}) - Q(s_t, a_t) \\big)$$\n",
    "\n",
    "With the tabular method, you're only updating $Q(s_t, a_t)$, but with the DQN approach this update changes slightly and the whole network updates at each step. When the whole network updates, your estimate of the best action at the next state ($max_a Q(s_{t+1})$) is changing too. This makes it difficult for the network to learn because the target is always moving meaning the error you're backpropagating isn't consistent from one update to the next for the same states and actions.\n",
    "\n",
    "To deal with this, DeepMind put in a **target network** which is a copy of the neural network we're training, but we only copy it after every $N$ time steps. This means that the error is going to be stable for some time allowing the network to learn, then the target network gets updated to a better approximation allowing us to begin learning again.\n",
    "\n",
    "### DQN Loss Function\n",
    "\n",
    "Because our Q-function is represented by a deep neural network, we need to change the update rule to make it applicable to backpropagation. This **loss function** needs to be differentiable with respect to the network's parameters ($\\theta$) as well. The logic for the DQN loss function is essentially the same as for the tabular Q-learning update rule: we take an action according to our Q-function and compare the reward we got with the estimate of our best action in this new state. \n",
    "\n",
    "$$\\mathcal{L}(\\theta) = \\bigg(R_t + \\gamma max_a \\big(Q(s_{t+1}; \\theta_t) \\big) - Q(s_t, a_t; \\theta) \\bigg)^2$$\n",
    "\n",
    "Again, we've got rewards and the Q-value estimates of our states and actions, but now we also have two networks given as $\\theta$ and $\\theta_t$ being our network and our target network respectively. We're squaring the error here in order to penalize large errors much more than smaller errors. To update the network, we apply the [backpropagation algorithm](https://www.datahubbs.com/deep-learning-101-the-theory/) which takes the derivative of all the values in the layers and makes changes to the corresponding layers. Thankfully, we've got PyTorch to do that for us!\n",
    "\n",
    "## Deep Q-Learning Algorithm with Experience Replay\n",
    "\n",
    "We'll be implementing the algorithm as found in the [DeepMind paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) (it's in the *Methods* section in case you want to read more on it yourself). Like for any deep reinforcement learning algorithm, we start by initializing our networks and environments, then loop through each episode getting data in the form of states, actions, and rewards. Because Q-learning is on-policy, we'll be updating our policy as we go, but this time using our experience replay buffer to draw samples from. We'll also use an $\\epsilon$-greedy selection strategy to choose which action to take next.\n",
    "\n",
    "> Initialize replay memory $D$ with $M$ samples and select minibatch sample size $B$\n",
    "\n",
    "> Initialize action-value function $Q$ and target network $Q_t$ with weights $\\theta$ and $\\theta_t = \\theta$\n",
    "\n",
    "> Select parameters $\\alpha, \\gamma \\in (0, 1]$\n",
    "\n",
    "> **FOR** each episode:\n",
    "\n",
    ">> Initialize $s_0$\n",
    "\n",
    ">>> **FOR** each step $t$ in the episode:\n",
    "\n",
    ">>>> **IF** $p < \\epsilon$ select a random action $a_t$\n",
    "\n",
    ">>>> **ELSE** select $argmax_a \\big(Q(s_t; \\theta) \\big)$\n",
    "\n",
    ">>>> Take action $a_t$ and observe reward $R_t$ and new state $s_{t+1}$\n",
    "\n",
    ">>>> Store transition ($s_t$, $a_t$, $R_t$, $s_{t+1}$) in replay buffer $D$\n",
    "\n",
    ">>>> Sample random minibatch of $B$ transitions from $D$\n",
    "\n",
    ">>>> Calculate the loss for all samples: $$\\mathcal{L}(\\theta) = \n",
    "\\begin{cases}\n",
    "  \\bigg(R_t + \\gamma max_a \\big(Q(s_{t+1}; \\theta_t) \\big) - Q(s_t, a_t; \\theta) \\bigg)^2 \\\\    \n",
    "  \\bigg(R_t - Q(s_t, a_t; \\theta) \\bigg)^2 \\quad \\text{if } s_{t+1} \\text{ is a terminal state}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    ">>>> Update parameters $\\theta$ with gradient descent\n",
    "\n",
    ">>>> Every $N$ steps, set $\\theta_t = \\theta$\n",
    "\n",
    "This is just like the [Q-learning](https://www.datahubbs.com/intro-to-q-learning/) algorithm except with the aforementioned modifications to accommodate the neural network and to incorporate experience replay. \n",
    "\n",
    "## DQN Implementation\n",
    "\n",
    "To implement everything just like DeepMind did, i.e. a network that is capable of playing most of the Atari-57 suite at or above human level, is going to take a lot of GPU computing resources; resources that many people don't have access to. For that reason, we'll simplify things a bit and only focus on the easier `CartPole` task in this post, and then in the follow-up, we'll add on the convolutional layers up front to learn from the raw pixels themselves. \n",
    "\n",
    "I've covered the [basics of `CartPole`](https://www.datahubbs.com/policy-gradients-with-reinforce/) before, so check out the link if you're not familiar with this environment. It's used because it's easy to work with to debug your algorithm and check your understanding before moving onto more challenging environments. \n",
    "\n",
    "In case you need to install the Gym environments, it's just:\n",
    "\n",
    "`pip install gym`\n",
    "\n",
    "Let's import a few of our basic packages and move on to the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import namedtuple, deque\n",
    "from copy import deepcopy, copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Network\n",
    "\n",
    "To start things off, we'll put the DQN together. We'll build our network using the `QNetwork` class which will assemble our network in PyTorch and enable us to get our actions and Q-values with the appropriate methods. All you need to pass here is the specific Gym environment you're working with, and it will be up and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, env, learning_rate=1e-3, device='cpu'):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.device = device\n",
    "        self.n_inputs = env.observation_space.shape[0]\n",
    "        self.n_outputs = env.action_space.n\n",
    "        self.actions = np.arange(env.action_space.n)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Set up network\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.n_inputs, 16, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16, bias=True),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(16, 16, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, self.n_outputs, bias=True))\n",
    "        \n",
    "        # Set to GPU if cuda is specified\n",
    "        if self.device == 'cuda':\n",
    "            self.network.cuda()\n",
    "            \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                          lr=self.learning_rate)\n",
    "        \n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            action = self.get_greedy_action(state)\n",
    "        return action\n",
    "    \n",
    "    def get_greedy_action(self, state):\n",
    "        qvals = self.get_qvals(state)\n",
    "        return torch.max(qvals, dim=-1)[1].item()\n",
    "    \n",
    "    def get_qvals(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array([np.ravel(s) for s in state])\n",
    "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
    "        return self.network(state_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DQN consists of a series of feed-forward layers leading to the output layer which will provide the estimated Q-value. To get a given action out of the network we call the `get_action()` method, which implements our $\\epsilon$-greedy strategy. There are also a few helper functions here such as `get_qvals()` and `get_greedy_action()` which return the Q-values of the given state and our estimate of the best action at a given state respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Memory Bank\n",
    "\n",
    "For the experience replay piece, we'll construct an `experienceReplayBuffer` to contain our transitions. We can use a `namedtuple` to store the values and load those into a `deque`, which is an object that stores values up to a limit. Once the `deque` limit is crossed, it tosses the first value out to make room for the new transition. This is going to be important for us because we want to keep updating our memory and, as the network learns, we expect newer transitions to be more relevant than older transitions because we should be in better positions. Plus, we don't want to continue growing our replay memory as that will take more and more computing resources as we train.\n",
    "\n",
    "This class also contains functions for appending values to the buffer and sampling it randomly. The `burn_in` is used to initialize our memory. If the `burn_in` value is set to 10,000, then at the beginning of training, we'll have the network take 10,000 completely random steps to populate the buffer so that we have sufficient values to train on. This will help get a wider variety of data to work with then we'd get if we set the `burn_in` value to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experienceReplayBuffer:\n",
    "\n",
    "    def __init__(self, memory_size=50000, burn_in=10000):\n",
    "        self.memory_size = memory_size\n",
    "        self.burn_in = burn_in\n",
    "        self.Buffer = namedtuple('Buffer', \n",
    "            field_names=['state', 'action', 'reward', 'done', 'next_state'])\n",
    "        self.replay_memory = deque(maxlen=memory_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        samples = np.random.choice(len(self.replay_memory), batch_size, \n",
    "                                   replace=False)\n",
    "        # Use asterisk operator to unpack deque \n",
    "        batch = zip(*[self.replay_memory[i] for i in samples])\n",
    "        return batch\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.replay_memory.append(\n",
    "            self.Buffer(state, action, reward, done, next_state))\n",
    "\n",
    "    def burn_in_capacity(self):\n",
    "        return len(self.replay_memory) / self.burn_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have a few helper functions as well, namely `sample_batch()` and `append()`. `sample_batch()` takes a batch size - let's say 32 in this case - and randomly selects 32 `(state, action, reward, done, next_state)` tuples from the memory and sticks those together into a single batch of 32. This batch is what we'll use to update our network. Of course, we need to add data to the buffer, and that's done using `append()`. The only thing to keep in mind with these is the order we construct our tuples in. `burn_in_capacity()` simply returns the ratio between the data in the buffer and the burn in limit allowing a quick check to see if we've satisfied that basic requirement before training the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tying it All Together\n",
    "\n",
    "We need to stick the DQN, replay buffer, and environment together in order to train. For this, we'll create one final class called `DQNAgent` which will take these three pieces as arguments along with a few other parameters, and enable training of the network. I'll refer to this as the **agent** for brevity because it is the object that is going to make decisions using the network and take those actions in the environment. \n",
    "\n",
    "Take a look at the code below, and then we'll walk through it for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, env, network, buffer, epsilon=0.05, batch_size=32):\n",
    "        \n",
    "        self.env = env\n",
    "        self.network = network\n",
    "        self.target_network = deepcopy(network)\n",
    "        self.buffer = buffer\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.window = 100\n",
    "        self.reward_threshold = 195 # Avg reward before CartPole is \"solved\"\n",
    "        self.initialize()\n",
    "    \n",
    "    def take_step(self, mode='train'):\n",
    "        if mode == 'explore':\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = self.network.get_action(self.s_0, epsilon=self.epsilon)\n",
    "            self.step_count += 1\n",
    "        s_1, r, done, _ = self.env.step(action)\n",
    "        self.rewards += r\n",
    "        self.buffer.append(self.s_0, action, r, done, s_1)\n",
    "        self.s_0 = s_1.copy()\n",
    "        if done:\n",
    "            self.s_0 = env.reset()\n",
    "        return done\n",
    "        \n",
    "    # Implement DQN training algorithm\n",
    "    def train(self, gamma=0.99, max_episodes=10000, \n",
    "              batch_size=32,\n",
    "              network_update_frequency=4,\n",
    "              network_sync_frequency=2000):\n",
    "        self.gamma = gamma\n",
    "        # Populate replay buffer\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            self.take_step(mode='explore')\n",
    "            \n",
    "        ep = 0\n",
    "        training = True\n",
    "        while training:\n",
    "            self.s_0 = self.env.reset()\n",
    "            self.rewards = 0\n",
    "            done = False\n",
    "            while done == False:\n",
    "                done = self.take_step(mode='train')\n",
    "                # Update network\n",
    "                if self.step_count % network_update_frequency == 0:\n",
    "                    self.update()\n",
    "                # Sync networks\n",
    "                if self.step_count % network_sync_frequency == 0:\n",
    "                    self.target_network.load_state_dict(\n",
    "                        self.network.state_dict())\n",
    "                    self.sync_eps.append(ep)\n",
    "                    \n",
    "                if done:\n",
    "                    ep += 1\n",
    "                    self.training_rewards.append(self.rewards)\n",
    "                    self.training_loss.append(np.mean(self.update_loss))\n",
    "                    self.update_loss = []\n",
    "                    mean_rewards = np.mean(\n",
    "                        self.training_rewards[-self.window:])\n",
    "                    self.mean_training_rewards.append(mean_rewards)\n",
    "                    print(\"\\rEpisode {:d} Mean Rewards {:.2f}\\t\\t\".format(\n",
    "                        ep, mean_rewards), end=\"\")\n",
    "                    \n",
    "                    if ep >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "                    if mean_rewards >= self.reward_threshold:\n",
    "                        training = False\n",
    "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                            ep))\n",
    "                        break\n",
    "                        \n",
    "    def calculate_loss(self, batch):\n",
    "        states, actions, rewards, dones, next_states = [i for i in batch]\n",
    "        rewards_t = torch.FloatTensor(rewards).to(device=self.network.device)\n",
    "        actions_t = torch.LongTensor(np.array(actions)).reshape(-1,1).to(\n",
    "            device=self.network.device)\n",
    "        dones_t = torch.ByteTensor(dones).to(device=self.network.device)\n",
    "        \n",
    "        qvals = torch.gather(self.network.get_qvals(states), 1, actions_t)\n",
    "        qvals_next = torch.max(self.target_network.get_qvals(next_states),\n",
    "                               dim=-1)[0].detach()\n",
    "        qvals_next[dones_t] = 0 # Zero-out terminal states\n",
    "        expected_qvals = self.gamma * qvals_next + rewards_t\n",
    "        loss = nn.MSELoss()(qvals, expected_qvals.reshape(-1,1))\n",
    "        return loss\n",
    "    \n",
    "    def update(self):\n",
    "        self.network.optimizer.zero_grad()\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size)\n",
    "        loss = self.calculate_loss(batch)\n",
    "        loss.backward()\n",
    "        self.network.optimizer.step()\n",
    "        if self.network.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())\n",
    "        \n",
    "    def initialize(self):\n",
    "        self.training_rewards = []\n",
    "        self.training_loss = []\n",
    "        self.update_loss = []\n",
    "        self.mean_training_rewards = []\n",
    "        self.sync_eps = []\n",
    "        self.rewards = 0\n",
    "        self.step_count = 0\n",
    "        self.s_0 = self.env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class starts with loading the network and environment as attributes to be called by other methods as well as setting up some empty lists and values to store data and count (see `initialize()`). We will use a `window` value of 100, which just means that we're going to average over the past 100 episodes to guage our progress to be consistent with the [OpenAI performance definitions](https://gym.openai.com/evaluations/eval_NQ6cZZXTR7y4EJQNV6HOA/) which focus on 100-episode averages. For `CartPole-v0`, the environment is considered 'solved' after an average reward of 195 is reached over a 100-episode span. \n",
    "\n",
    "The primary method is `train()` which allows us to specify the discount factor `gamma`, the maximum number of episodes, our batch size, how often we update our network, and how regularly we synchronize our target network with out current, trained network. All of these parameters are taken and used to run our training loop where we first populate our `experienceReplayBuffer` with our burn-in length, then let the network take its actions and get feedback from the `experienceReplayBuffer`. \n",
    "\n",
    "We call the `take_step()` method both when training and populating the replay buffer. This takes an argument called `mode` which is either set to `train` or `explore`. If we set this to `explore`, we take random actions, so this is used for the burn-in phase. If we set it to `train`, then we recruit the neural network to implement our $\\epsilon$-greedy action selection. \n",
    "\n",
    "If you keep walking through the training loop, you'll notice that we have our network updating and network synchronization steps next. In the *DeepMind* paper, they set the `network_update_frequency=4`, meaning they only sample from their replay buffer and run the backpropogation algorithm after every four training steps. This is the computationally expensive part of the algorithm, so if we can push that number higher, we'll get an algorithm that will run through more episodes faster, but that doesn't necessarily mean that it will train any faster. \n",
    "\n",
    "Synching the network isn't particularly computationally expensive, however, because we use the target network to determine our error, if we update too often, then our network is chasing a moving target. Providing some stability here for a few episodes enables it to minimize the error, then learn the new target. This yields steadier results in the training process. \n",
    "\n",
    "Other key methods are `update()` and `calculate_loss()`. We'll look at these in a bit more detail because they are key to training the algorithm and similar functions appear all throughout deep learning work.\n",
    "\n",
    "First, let's start with `update()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self):\n",
    "    self.network.optimizer.zero_grad()\n",
    "    batch = self.buffer.sample_batch(batch_size=self.batch_size)\n",
    "    loss = self.calculate_loss(batch)\n",
    "    loss.backward()\n",
    "    self.network.optimizer.step()\n",
    "    if self.network.device == 'cuda':\n",
    "        self.update_loss.append(loss.detach().cpu().numpy())\n",
    "    else:\n",
    "        self.update_loss.append(loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I always start my PyTorch `update()` functions with `self.network.optimizer.zero_grad()`. This clears out any gradients in the network optimizer from any possible previous runs allowing us to calculate new gradients going forward. After that, we need to sample a batch from our replay buffer, and then calculate the loss according to our loss function. When we have the `loss`, we differentiate to get the gradients by calling `loss.backward()` and then apply these gradients to the network by calling `self.network.optimizer.step()`. Finally, we store those loss values in the `self.update_loss` list. If we're running it on a GPU system, then we have to send it to the CPU first. But it's always important to call `detach()` on your loss so that PyTorch no longer associates these values with your computational graph.\n",
    "\n",
    "The other key function here is `calculate_loss()`. It showed up in the `update()` method and returns those key loss values for us to train with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(self, batch):\n",
    "    states, actions, rewards, dones, next_states = [i for i in batch]\n",
    "    rewards_t = torch.FloatTensor(rewards).to(device=self.network.device)\n",
    "    actions_t = torch.LongTensor(np.array(actions)).reshape(-1,1).to(\n",
    "        device=self.network.device)\n",
    "    dones_t = torch.ByteTensor(dones).to(device=self.network.device)\n",
    "\n",
    "    qvals = torch.gather(self.network.get_qvals(states), 1, actions_t)\n",
    "    qvals_next = torch.max(self.target_network.get_qvals(next_states),\n",
    "                           dim=-1)[0].detach()\n",
    "    qvals_next[dones_t] = 0 # Zero-out terminal states\n",
    "    expected_qvals = self.gamma * qvals_next + rewards_t\n",
    "    loss = nn.MSELoss()(qvals, expected_qvals.reshape(-1,1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, we have to unpack the our batch from the replay memory buffer and seperate these into separate arrays. Then for the rewards, actions, and done values, they need to be converted to PyTorch tensors and shaped properly (the states are converted by the network when we call `get_qvals()`, so we don't need to do that twice). `get_qvals()` returns an array of Q-values with the dimensions `[batch_size, number of actions]` (or $32 \\times 2$ in this case). To update our network, we need to get the Q-values for the actions that we actually took, so we use PyTorch's `gather()` function to subset our Q-values appropriately. \n",
    "\n",
    "Similarly, because our loss function compares our actual action with the discounted best estimate from the next state $\\big(\\gamma max_a (Q(s_{t+1}; \\theta_t) \\big)$, we call `get_qvals()` from our target network and pass the `next_states` array. We use PyTorch's `max()` function to select the maximum of each row (set with `dim=-1`). This function returns a tuple of the maximum value and the index of the maximum value, so we use `[0]` just to get the max value for each row and we call `detach()` to ensure that these values don't update our target network when we call `loss.backward()` and `optimizer.step()`. The `qvals_next` are set to 0 at terminal states because there are no future rewards to discount. Then we combine all of these together with the reward and the discount factor to get the `expected_qvals`. Finally, the loss is calculated as the mean squared error of the Q-values of the actions we took minus the expected, discounted Q-values from our target network and the loss is returned to the update function.\n",
    "\n",
    "There's a lot there, but all of it was just to calculate the loss according to:\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = \n",
    "\\begin{cases}\n",
    "  \\bigg(R_t + \\gamma max_a \\big(Q(s_{t+1}; \\theta_t) \\big) - Q(s_t, a_t; \\theta) \\bigg)^2 \\\\    \n",
    "  \\bigg(R_t - Q(s_t, a_t; \\theta) \\bigg)^2 \\quad \\text{if } s_{t+1} \\text{ is a terminal state}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, I think we're finally ready to train the network! We do that by initializing the environment, buffer, DQN, and the agent, then calling `agent.train()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1396 Mean Rewards 195.10\t\t\n",
      "Environment solved in 1396 episodes!\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "buffer = experienceReplayBuffer(memory_size=10000, burn_in=1000)\n",
    "dqn = QNetwork(env, learning_rate=1e-3)\n",
    "agent = DQNAgent(env, dqn, buffer)\n",
    "agent.train(max_episodes=5000, network_update_frequency=1, \n",
    "            network_sync_frequency=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plotting the rewards:\n",
    "\n",
    "<img src=\"https://www.datahubbs.com/wp-content/uploads/2019/03/dqn_cartpole_training.png\">\n",
    "\n",
    "Hopefully this will train fairly quickly for you on a CPU. It may take a few tries and different hyperparameters to get it to train quickly, but it should work and be easy to de-bug. \n",
    "\n",
    "I pulled out the basics from the *DeepMind* paper on Deep Q-Learning, but there are a few more things to get into to see how their model worked and to be able to apply it to learning from raw pixels. I'll get into that next time where we'll add a few convolutional neural network layers to the front-end of our DQN to enable it to learn from the images we pass. In the meantime, if you want to play around with the Atari games, there are RAM versions which read data directly from the system and represent the game state as a vector of 128 numbers. You'll likely need a larger neural network, but the tutorial here will let you plug one of those in as your environment and let you play Atari now. Just use `pip install gym[atari]` and set your environment to something like `Breakout-ram-v0` and give it a shot!\n",
    "\n",
    "Also, if you want to get some more versatile code that you can run from the command line, check out the `dqn.py` file [here on GitHub](https://github.com/hubbs5/rl_blog/blob/master/q_learning/deep/dqn.py)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
