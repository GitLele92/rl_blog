{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting started with deep reinforcement learning is not easy. \n",
    "\n",
    "There are a host of challenges from the different terminology versus supervised learning or optimization, to developing a simulation, and of course, the alphabet soup of algorithms to choose from and the Greek alphabet soup of hyperparameters to fiddle with.\n",
    "\n",
    "Moreover, RL tends to be *extremely* data hungry, requiring thousands if not millions of simulations in order to learn a good policy. Even if you don't mind jumping in to the papers and implementing the algorithm yourself, you'll find that optimizing the algorithms and taking advantage of parallelization is going to be important to getting results.\n",
    "\n",
    "That's where [Ray](https://ray.readthedocs.io/en/latest/index.html) comes in. Ray has been around since 2017, developed by [UC Berkeley's RISE Lab](https://rise.cs.berkeley.edu/), it is designed to bring scalable, parallelizable, reinforcement learning to practitioners and researchers without the pain of implementing models yourself.\n",
    "\n",
    "<img src=\"https://www.datahubbs.com/wp-content/uploads/2020/04/ray_header_logo.png\">\n",
    "\n",
    "Before we jump into the RL piece, let's spend some time with the basics of Ray and show how we can use it to achieve speed-ups in our computations via parallel computing.\n",
    "\n",
    "## TL;DR\n",
    "\n",
    "We introduce Ray and show how to parallelize a few different functions to get increased performance from the library.\n",
    "\n",
    "## First Parallelization with Ray\n",
    "\n",
    "Before we get into the full-blown RL models, I want to walk through some simpler examples of parallelization and explain some of the benefits of the technique.\n",
    "\n",
    "Typically, programs you write in Python are done in serial, i.e. one step after the next. In many applications this is just fine. However, given that modern machines - be it your laptop or an AWS server - has multiple CPU's, you can take advantage of that by breaking problems down so they can be run simultaneously in parallel to get huge speed-ups!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.datahubbs.com/wp-content/uploads/2020/04/parallel_computing.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the case for many machine learning algorithms, and particularly for reinforcement learning where Monte Carlo simulation is employed to generate training data. These simulations can be run in parallel and the trajectories sent back to the neural network for updating. This is incredibly useful and can greatly speed up your training.\n",
    "\n",
    "### Parallelizing a Timer\n",
    "\n",
    "To get started, we need to install Ray using `pip install ray`. One thing to note, as of this writing, Ray will only run on Linux and MacOs machines, and is only compatible for Python 3.5-3.7 ([check the docs for updates](https://ray.readthedocs.io/en/latest/installation.html)). If your machine doesn't meet those requirements, then you can hop over to [Google Colab](https://colab.research.google.com) for free access to a notebook where you can run this code.\n",
    "\n",
    "Let's show an example of the speed-up's for parallelization by starting with a sequential program, timing it, and moving it to Ray. In the first instance, we're going to take a standard timing example to show the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a timer function that takes an argument, `x`, waits 1 second, then returns `x`. This is utterly useless, but will illustrate the sequential versus parallel power we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(x):\n",
    "    time.sleep(1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, timing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed:\t4.0043\n",
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "values = [timer(x) for x in range(4)]\n",
    "print('Time Elapsed:\\t{:.4f}'.format(time.time() - t0))\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we parallelize, we need to initialize Ray with `ray.init()`, where we can set the number of CPU's we have. If you don't know what you have available, just run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-03 11:55:09,476\tWARNING services.py:597 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2020-04-03 11:55:09,481\tINFO resource_spec.py:216 -- Starting Ray with 1.61 GiB memory available for workers and up to 0.82 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()\n",
    "ray.available_resources()['CPU']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see above, my machine has 8 CPU's I can use to parallize my process. If I don't pass a specific value when I call `ray.init`, it will use all 8. I'm going to reinitialize `ray` with only 4 CPU's available to it so that each CPU will handle one of the calls to our timer function independently. Note that if you reinitialize ray from and IDE like Jupyter, you must pass the `ignore_reinit_error=True` argument, otherwise you'll get an error, or you need to restart your kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-03 12:01:06,559\tERROR worker.py:679 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "ray.init(num_cpus=4, ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parallelize our function with Ray, we just need to decorate it with `remote`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def timer_ray(x):\n",
    "    time.sleep(1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the exact same code as above, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed:\t0.0025\n",
      "[ObjectID(7dec8564195ad979ffffffff010000c801000000), ObjectID(0bead116322a6c2bffffffff010000c801000000), ObjectID(b944ee5bb38dd1a5ffffffff010000c801000000), ObjectID(2a124e2070438a75ffffffff010000c801000000)]\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "values = [timer_ray.remote(x) for x in range(4)]\n",
    "print('Time Elapsed:\\t{:.4f}'.format(time.time() - t0))\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully the above looks odd to you. First, the elapsed time is not the expected 1 second, and second, the results look like a lot of gibberish. What Ray is doing here is measuring the time it takes to create object ID's to be run, not the time it takes to run the code itself. That's what we're seeing when we print the `values` list: a list of object ID's that point to these tasks. To get Ray to actually evaluate these functions, we need to call `ray.get()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.get(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to get the timing and expected results for all of this, we'll wrap our list in the `ray.get()` function and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed:\t1.0106\n",
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "values = ray.get([timer_ray.remote(x) for x in range(4)])\n",
    "print('Time Elapsed:\\t{:.4f}'.format(time.time() - t0))\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the expected output!\n",
    "\n",
    "Let's turn to a more useful example to show how we can leverage this in actual computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelizing Moving Averages\n",
    "\n",
    "There are numerous financial measurements and strategies that require computing moving averages. Sometimes you want a simple, 90-day moving average, other times a 10-day, or some other value. That's not so bad if you only have a few time series to deal with. But, as is often the case, you might need to calculate simple moving averages for thousands of different securities on a regular basis. If this is the case, we can take advantage of parallelization and make great gains.\n",
    "\n",
    "Let's start by generating 1,000 different, random time series to show how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.normal(size=(1000, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we'll implement a sequential simple moving average calculation that returns a 10-day moving average. If too little data is available - such as over the first 9 data points - it will just give the moving average over those days.\n",
    "\n",
    "The function below will give us the desired result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_moving_average(data, window=10):\n",
    "    ma_data = np.zeros(data.shape)\n",
    "    for i, row in enumerate(data):\n",
    "        ma_data[i] = np.array(\n",
    "            [np.mean(row[j-window:j+1]) \n",
    "             if j > window else np.mean(row[:j+1]) \n",
    "             for j, _ in enumerate(row)])        \n",
    "    return ma_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `calc_moving_average` function takes each individual time series (denoted by a row in our data) and then returns the moving average for each step. If you plot this, it will show a smoothed value.\n",
    "\n",
    "We'll time it as we did for the examples above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed:\t7.9067\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "ma_data = calc_moving_average(data)\n",
    "seq_time = time.time() - t0\n",
    "print('Time Elapsed:\\t{:.4f}'.format(seq_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly 8 seconds elapse to calculate this. Let's see if we can do better by using our `@ray.remote` decorator on the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def calc_moving_average_ray(data, window=10):\n",
    "    ma_data = np.zeros(data.shape)\n",
    "    for i, row in enumerate(data):\n",
    "        ma_data[i] = np.array(\n",
    "            [np.mean(row[j-window:j+1]) \n",
    "             if j > window else np.mean(row[:j+1]) \n",
    "             for j, _ in enumerate(row)])        \n",
    "    return ma_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed:\t7.6218\n",
      "Speed up:\t1.0X\n",
      "Results match:\tTrue\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "ma_data = ray.get(calc_moving_average_ray.remote(data))\n",
    "par_time = time.time() - t0\n",
    "print('Time Elapsed:\\t{:.4f}'.format(par_time))\n",
    "print('Speed up:\\t{:.1f}X'.format(seq_time / par_time))\n",
    "print(\"Results match:\\t{}\".format(np.allclose(ma_data, ma_data_ray)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our implementation didn't work out so well versus the baseline: we saved ~0.3 seconds, not quite the boost we were looking for. The reason for this is that we didn't break the moving average calculation down into easily parallelizable steps. We just told the computer to parallelize the entire algorithm, not the pieces that make the most sense. \n",
    "\n",
    "We can make a slight adjustment to this as shown below and see what the speed-up looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def calc_moving_average_ray(row, window=10):\n",
    "    return np.array([np.mean(row[j-window:j+1]) \n",
    "             if j > window else np.mean(row[:j+1]) \n",
    "             for j, _ in enumerate(row)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed:\t2.2801\n",
      "Speed up:\t3.5X\n",
      "Results match:\tTrue\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "ma_data_ray = np.array(ray.get(\n",
    "    [calc_moving_average_ray.remote(row) \n",
    "    for row in data]\n",
    "    ))\n",
    "par_time = time.time() - t0\n",
    "print('Time Elapsed:\\t{:.4f}'.format(par_time))\n",
    "print('Speed up:\\t{:.1f}X'.format(seq_time/par_time))\n",
    "print(\"Results match:\\t{}\".format(np.allclose(ma_data, ma_data_ray)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a 3.5X speed up, which is about what we'd expect now that we've parallelized our process over 4 CPU's rather than running on a single processor. All we had to change was how we're inputting our data to the function. By passing each row of the data to the function, we've parallelized at a lower and more meaningful level for this algorithm. \n",
    "\n",
    "We don't get an exact 4.0X speed up because there is some overhead cost associated with this operation. Typically, the more information that needs to be moved around, the more overhead cost we incur. This means we want to shy away from lots of small operations, because it might cost more in terms of time to pass the information back and forth between the various cores than we gain by parallelizing.\n",
    "\n",
    "## Ray for RL\n",
    "\n",
    "Ray has two other libraries built on top of it, `RLLIB` and `Tune`, both of which are incredibly powerful for implementing reinforcement learning algorithms. They take advantage of the parallelization we discussed here and I'll turn to introducing those libraries and key functionalities in subsequent posts. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
